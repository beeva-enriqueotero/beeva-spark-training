{
 "metadata": {
  "name": "ex1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Spark exercises"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## README\nDownload Spark binaries [here](http://spark.apache.org/downloads.html) or directly\n[here](http://www.apache.org/dyn/closer.cgi/spark/spark-1.2.0/spark-1.2.0-bin-hadoop2.4.tgz)\n\nResources:\n\n+ [Spark Programming Guide (v1.2.0)](http://spark.apache.org/docs/latest/programming-guide.html)\n\n+ [Spark Python API Docs](http://spark.apache.org/docs/latest/api/python/index.html)\n\n+ [pyspark-pictures](http://nbviewer.ipython.org/github/jkthompson/pyspark-pictures/blob/master/pyspark-pictures.ipynb) : Learn the pyspark API through pictures and simple examples (thanks to *jkthompson*)\n\n+ [Databricks training resources](http://databricks.com/spark-training-resources)\n\n+ Spark on AWS, exercises applied to NLP (thanks to *utcompling*): [Ex1](https://github.com/utcompling/applied-nlp/wiki/Spark-AWS-Exercise1) [Ex2](https://github.com/utcompling/applied-nlp/wiki/Spark-AWS-Exercise2)\n\n+ [Submitting applications](http://spark.apache.org/docs/latest/submitting-applications.html)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Hello Scala!\n\nLaunch spark-shell (localhost, driver/master only): \n``\n./bin/spark-shell --master local[4]\n``\n\nEnter ```\nsc.getConf.toDebugString\n```\n\nOpen SparkContext web UI in [localhost:4040](http://localhost:4040)\n\nLaunch 1-machine \"cluster\" in localhost:\n``\n./sbin/start-all.sh\n``\n\nOpen Spark master's web UI in [localhost:8080](http://localhost:8080)\n\nLaunch spark-shell (localhost, driver and worker): \n``\n./bin/spark-shell --master spark://<IP>:7077\n``"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##Good Bye Scala! Hello Python!\n\nLaunch pyspark shell (with ipython notebook support):\n``\nPYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" ~/herramientas/spark-1.2.0-bin-hadoop2.4/bin/pyspark\n``"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Exercises"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from pyspark.context import SparkContext\nprint \"Running Spark Version %s\" % (sc.version)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Running Spark Version 1.2.0\n"
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from pyspark.conf import SparkConf\nmyconf = SparkConf()\nprint myconf.toDebugString()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "spark.app.name=pyspark-shell\nspark.master=local[*]\n"
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "data = [1, 2, 3, 4, 5]\ndistData = sc.parallelize(data)\ndistData",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:364"
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "distData.sum()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": "15"
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "distData.reduce(lambda a,b : a + b) ",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": "15"
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Configuration properties"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "sc.stop()",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "from pyspark.conf import SparkConf\nmyconf = SparkConf()\nmyconf.set('spark.driver.memory', '512m').set(\"spark.app.name\", \"My spark app\")\nprint myconf.toDebugString()\nsc = SparkContext(conf = myconf)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "spark.app.name=My spark app\nspark.driver.memory=512m\nspark.master=local[*]\n"
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "####Another way:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "SparkContext.setSystemProperty(\"spark.app.name\", \"My spark app 2\")\nSparkContext.setSystemProperty('spark.driver.memory', '512m')\nprint myconf.toDebugString()\nsc.stop()\nsc = SparkContext()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "spark.app.name=My spark app\nspark.driver.memory=512m\nspark.master=local[*]\nspark.rdd.compress=True\nspark.serializer.objectStreamReset=100\n"
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Working with text files"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}